# Given a FASTQ file generated by a long-read sequencer such as PacBio,
# this script aligns the reads to a reference genome, identifies structural variants (SVs),
# and summarizes the detected SVs with relevant information.
#
# Dependencies:
# 1) Input data:
#    - Reference genome in FASTA format
#    - Optional prebuilt Minimap2 index (.mmi) corresponding to the reference
#    - A FASTQ file generated by a long-read sequencer (e.g., PacBio)
#    - An annotation file in BED format (used to find which genes overlap each structural variant)
#
# 2) External tools (tested versions):
#    - seqkit    v2.10.1
#    - minimap2  v2.30-r1287
#    - samtools  v1.22.1
#    - sniffles  v2.6.3
#    - bedtools  v2.31.1
#
# 3) Python dependency (tested version):
#    - pysam     v0.23.3
#    - pybedtools
#
# TO RUN: python ./longread_sv_pipeline.py -f <FASTQ input file> -a <annotation BED> -r <reference FASTA> [-i <reference index>]
# For example: python ./longread_sv_pipeline.py -f ./TestData/brca1_with_sva_hifi.fastq -a ./TestData/chr17_annotations.bed -r ./TestData/chr17.fasta

import argparse
import subprocess
import shlex
import pysam
from pybedtools import BedTool
from collections import Counter
from datetime import datetime
from pathlib import Path

SCRIPT_DIR = Path(__file__).resolve().parent

class StructuralVariant:
    def __init__(self, chrom, start, end, sv_type, support, coverage, length=None):
        self.chrom = chrom
        self.start = start
        self.end = end
        self.sv_type = sv_type
        self.support = support
        self.length = length if length is not None else end - start

# Given a dictionary of FASTQ statistics,
# check basic properties to confirm that the file appears to be a valid
# long-read dataset.
def is_fastq_valid_longread(stats):
    num_reads = int(stats.get("num_seqs", 0).replace(",", ""))
    avg_len = float(stats.get("avg_len", 0).replace(",", ""))
    min_len = float(stats.get("min_len", 0).replace(",", ""))

    issues = []
    if num_reads == 0:
        issues.append("no reads found in FASTQ file")
    if avg_len < 1000:
        issues.append(f"average read length ({avg_len:.0f} bp) is unusually short for HiFi data")
    if min_len < 100:
        issues.append(f"some reads are extremely short (min length {min_len:.0f} bp)")
    
    if issues:
        return (False, "\n".join(issues))
    return (True, "")

# Print a summary of a FASTQ fileâ€™s statistics and return a dictionary of those values.
def summarize_fastq(fastq_path):
    result = run_command(
        ["seqkit", "stats", fastq_path],
        description="FASTQ Summary (seqkit stats)",
        capture_output=True
    )

    lines = result.stdout.strip().splitlines()

    header = lines[0].split()
    values = lines[1].split()

    stats = dict(zip(header, values))

    print(result.stdout.strip())
    
    return stats

# Align reads to the reference genome, then convert, sort, and index the results
def make_bam(reference_index, fastq_file, output_dir):
    reference_index = Path(reference_index).expanduser()
    fastq_file = Path(fastq_file).expanduser()
    output_dir = Path(output_dir).expanduser()
    output_dir.mkdir(parents=True, exist_ok=True)

    sam_path = output_dir / (fastq_file.stem + ".sam")
    bam_path = output_dir / (fastq_file.stem + ".sorted.bam")

    # Align reads to given reference
    run_command(
        ["minimap2", "-t", "8", "-ax", "map-hifi", str(reference_index), str(fastq_file), "-o", str(sam_path)],
        "Aligning reads with minimap2", capture_output=False, warn_about_long_process=True
    )
    # Convert sam to bam and sort
    cmd = (
        f"samtools view -bS {sam_path} | "
        f"samtools sort -o {bam_path}"
    )
    run_command(
        cmd,
        "Converting SAM to sorted BAM", capture_output=False, warn_about_long_process=True
    )
    cmd = [
        "samtools",
        "index",
        str(bam_path)
    ]
    run_command(cmd, f"Indexing BAM file: {bam_path.name}", capture_output=False, warn_about_long_process=True)

    return bam_path

# Run minimap2 to build an index from the provided reference genome
def build_minimap2_index(reference_fasta, output_index):
    result = run_command(
        ["minimap2", "-d", str(output_index), str(reference_fasta)],
        description="Building Minimap2 index",
        capture_output=False
    )

def make_vcf(fastq, bam, outputs_dir, ref):
    vcf = Path(outputs_dir) / (Path(fastq).stem + ".sv.vcf")

    cmd = [
        "sniffles",
        "--input", str(bam),
        "--vcf", str(vcf),
        "--reference", str(ref),
        "--threads", "8"
    ]
    result = run_command(cmd, "Running Sniffles for structural variant detection", capture_output=False)
    print((result.stdout or "") + (result.stderr or ""))

    return vcf

# Returns a BedTool object containing the given structural variants
def get_svs_as_bed(svs):
    lines = []
    for svtype, sv_list in svs.items():
        for sv in sv_list:
            # BED format: chrom, start, end, name (svtype)
            lines.append(f"{sv.chrom}\t{sv.start}\t{sv.end}")
    return BedTool("\n".join(lines) + "\n", from_string=True)

# Displays details for each structural variant - such as type, length, and coverage - along with overlapping genes
def summarize_svs(svs, svs_bed, annotation_file):
    annotation = BedTool(annotation_file)
    overlaps = svs_bed.intersect(annotation, wa=True, wb=True)
    for feature in overlaps:
        print(feature)

# Given a VCF and BAM file, returns a dict of SV objects representing structural variants with sufficient read coverage.
def get_svs(vcf_path, bam_path):
    vcf = pysam.VariantFile(vcf_path)

    cmd = (
        f"samtools depth {bam_path} | awk '{{sum+=$3; n++}} END {{if(n>0) print sum/n; else print 0}}'"
    )
    result = run_command(cmd, "Calculating average coverage (nonzero positions only)", capture_output=True)
    average_coverage = float(result.stdout.strip())

    svs = {}
    for rec in vcf.fetch():
        svtype = rec.info.get("SVTYPE", "?")
        svlen_field = rec.info.get("SVLEN", 0)
        svlen = svlen_field[0] if isinstance(svlen_field, (list, tuple)) else svlen_field
        support_field = rec.info.get("SUPPORT", 0)
        support = support_field[0] if isinstance(support_field, (list, tuple)) else support_field

        depth_field = rec.info.get("COVERAGE", "NA")
        coverage = depth_field[0] if isinstance(depth_field, (list, tuple)) else depth_field

        # Only retain SVs with coverage greater than 25% of the average coverage across all detected SVs.
        # This helps filter out likely noise and false positives. Such low-coverage SVs can occur when
        # using a partial reference: reads originating from outside the target region may be forced to
        # align imperfectly to the available reference, producing false variant calls.
        if coverage > average_coverage * .25:
            new_sv = StructuralVariant(rec.chrom, rec.pos, rec.pos + svlen, svtype, support, coverage, svlen)
            svs.setdefault(svtype, []).append(new_sv)
    return svs

def summarize_svs_old(vcf_path, bam_path, annotation):
    vcf = pysam.VariantFile(vcf_path)
    sv_counts = Counter()

    cmd = (
        f"samtools depth {bam_path} | awk '{{sum+=$3; n++}} END {{if(n>0) print sum/n; else print 0}}'"
    )
    result = run_command(cmd, "Calculating average coverage (nonzero positions only)", capture_output=True)
    average_coverage = float(result.stdout.strip())

    print(f"\n*********** Structural Variant Summary ***********")
    print(f"{'CHROM':<18} {'POS':<10} {'TYPE':<6} {'LEN':<8} {'SUPPORT':<8} {'COVERAGE':<8}")
    print("-" * 70)

    for rec in vcf.fetch():
        svtype = rec.info.get("SVTYPE", "?")

        svlen_field = rec.info.get("SVLEN", 0)
        svlen = svlen_field[0] if isinstance(svlen_field, (list, tuple)) else svlen_field
        support_field = rec.info.get("SUPPORT", 0)
        support = support_field[0] if isinstance(support_field, (list, tuple)) else support_field

        depth_field = rec.info.get("COVERAGE", "NA")
        coverage = depth_field[0] if isinstance(depth_field, (list, tuple)) else depth_field

        sv_counts[svtype] += 1

        # Only retain SVs with coverage greater than 25% of the average coverage across all detected SVs.
        # This helps filter out likely noise and false positives. Such low-coverage SVs can occur when
        # using a partial reference: reads originating from outside the target region may be forced to
        # align imperfectly to the available reference, producing false variant calls.
        if coverage > average_coverage * .25:
            print(f"{rec.chrom:<18} {rec.pos:<10} {svtype:<6} {svlen:<8} {support:<8} {coverage:<8}")


    print("\n*********** Counts by Type ***********")
    for svtype, count in sv_counts.items():
        print(f"{svtype}: {count}")

def run_command(cmd, description, capture_output=False, warn_about_long_process=False):
    print(f"\n\n*********** {description} ***********")
    if isinstance(cmd, list):
        print(f"Command: {shlex.join(map(str, cmd))}")
    else:
        print(f"Command: {cmd}")
    if warn_about_long_process:
        print(f"Please be patient. This process can take a few minutes.")

    try:
        result = subprocess.run(
            cmd,
            shell=isinstance(cmd, str),
            check=True,
            text=True,
            capture_output=capture_output
        )
        return result
    except subprocess.CalledProcessError as e:
        print(f"Error: {e}")
        if e.stderr:
            print(e.stderr)
        exit(1)
    except FileNotFoundError:
        print(f"Command not found: {cmd[0]}. Make sure it's installed and in your PATH.")
        exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description=(
            "A pipeline for detecting structural variants in long-read sequencing data."
        )
    )
    parser.add_argument(
        "-r", "--reference",
        type=Path,
        required=True,
        help="Path to the reference genome in FASTA format."
    )
    parser.add_argument(
        "-f", "--fastq",
        type=Path,
        required=True,
        help="Path to the sample data FASTQ file generated by a long-read sequencer."
    )
    parser.add_argument(
        "-i", "--index",
        type=Path,
        required=False,
        help="Optional path to a prebuilt minimap2 index (.mmi). "
             "If not provided, the index will be generated automatically."
    )
    parser.add_argument(
         "-a", "--annotation",
        type=Path,
        required=True,
        help=(
        "Path to a BED file containing genomic annotations corresponding to the reference genome. "
        "This file is used to identify which genes overlap detected structural variants.")
    )
    args = parser.parse_args()

    # Verify that the FASTQ file appears to contain valid long-read data
    fastq_stats = summarize_fastq(args.fastq)
    is_valid = is_fastq_valid_longread(fastq_stats)
    if not is_valid[0]:
        response = input(f"Your input FASTQ file has some issues:\n{is_valid[1]}. \n\nDo you want to continue? (y/n)")
        if response != "y":
            exit()

    # Index the given reference genome, if the index has not been provided (save in the same directory as the reference genome)
    ref_index = args.index
    if not args.index:
        ref_index = args.reference.with_suffix(".mmi")
        build_minimap2_index(args.reference, ref_index)

    # Align the FASTQ, build a sorted BAM, find structural variants
    outputs_dir = SCRIPT_DIR / "Outputs" / datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    bam = make_bam(ref_index, args.fastq, outputs_dir)
    vcf = make_vcf(args.fastq, bam, outputs_dir, args.reference)

    # Summarize the SVs - their coordinates, type, support, coverage, overlapping genes, etc.
    svs = get_svs(vcf, bam)
    svs_bed = get_svs_as_bed(svs)
    summarize_svs(svs, svs_bed, args.annotation)

            
        
